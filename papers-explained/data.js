const nav_data = {
  "Language Models": ["Language Models"],
  "Transformer Based Language Models": [
    "Encoder Only Transformers", "Decoder Only Transformers", "Frontier Models", "Small LLMs", "Multi Modal LMs", "LLM for Math", "LLM Training", "Reasoning Models", "Reward Models", ],
  "Retrieval and Representation Learning": ["Retrieval and Representation Learning"],
  "Parameter Efficient Fine Tuning": ["PEFT"],
  "Image Models": ["Vision Transformers", "CNNs", "Object Detection", "RCNNs"],
  "Generative Models": ["Image Generation", "GANs", "Autoencoders"],
  "Document Understanding": ["Document Understanding", "Layout Aware LMs"],
  "Tabular Data": ["Tabular Data"],
  "Datasets": ["Datasets"],
  "Neural Network Layers": ["Neural Network Layers"],
  "Model Merging": ["Model Merging"],
  "Miscellaneous Studies": ["Miscellaneous Studies"],
  "Broad Studies": ["Surveys", "Journeys", "Literature Review"],
}

const all_classes = [
  "Language Models",
  "Encoder Only Transformers",
  "Decoder Only Transformers",
  "Frontier Models",
  "Small LLMs",
  "Multi Modal LMs",
  "LLM for Math",
  "LLM Training",
  "Reasoning Models",
  "Reward Models",
  "Retrieval and Representation Learning",
  "PEFT",
  "Vision Transformers",
  "CNNs",
  "Object Detection",
  "RCNNs",
  "Image Generation",
  "GANs",
  "Autoencoders",
  "Document Understanding",
  "Layout Aware LMs",
  "Tabular Data",
  "Datasets",
  "Neural Network Layers",
  "Model Merging",
  "Miscellaneous Studies",
];

const papers_data = [
    language_models,
    encoder_only_transformers,
    decoder_only_transformers,
    frontier_models,
    small_llms,
    multi_modal_lms,
    llm_for_math,
    llm_training,
    reasoning_models,
    reward_models,
    retrieval_and_representation_learning,
    peft,
    vision_transformers,
    convolutional_neural_networks,
    object_detection,
    rcnn,
    image_generation,
    generative_adversarial_networks,
    autoencoders,
    document_understanding,
    layout_aware_language_models,
    tabular_data,
    datasets,
    neural_network_layers,
    model_merging,
    miscellaneous_studies,
];

const surveys_data = [
  {
    title: "Best Practices and Lessons Learned on Synthetic Data",
    link: "2404.07503",
    date: "April 2024",
    description:
      "Provides an overview of synthetic data research, discussing its applications, challenges, and future directions.",
    tags: ["Survey", "Synthetic Data"],
  },
  {
    title: "The Prompt Report: A Systematic Survey of Prompting Techniques",
    link: "2406.06608",
    date: "June 2024",
    description:
      "Establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use.",
    tags: ["Survey", "Prompt Optimization"],
  },
  {
    title: "What is the Role of Small Models in the LLM Era",
    link: "2409.06857",
    date: "September 2024",
    description:
      "Systematically examines the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition.",
    tags: ["Survey", "Small Models"],
  },
  {
    title: "Small Language Models: Survey, Measurements, and Insights",
    link: "2409.15790",
    date: "September 2024",
    description:
      "Surveys 59 SoTA open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms, evaluate their capabilities, and benchmark their inference latency and memory footprints.",
    tags: ["Survey", "Small Models"],
  },
  {
    title: "A Survey of Small Language Models",
    link: "2410.20011",
    date: "October 2024",
    description:
      "A comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques",
    tags: ["Survey", "Small Models"],
  },
];

const journeys_data = [
  {
    title: "Encoder Only Transformers",
    link: "transformer-encoders",
    papers: [
      "BERT",
      "RoBERTa",
      "Sentence BERT",
      "Tiny BERT",
      "ALBERT",
      "Distil BERT",
      "Distil RoBERTa",
      "FastBERT",
      "MobileBERT",
      "ColBERT",
      "DeBERTa",
      "DeBERTa v2",
      "DeBERTa v3",
      "ColBERT v2",
    ],
  },
  {
    title: "Vision Transformers",
    link: "vision-transformers",
    papers: [
      "Vision Transformer",
      "What do Vision Transformers Learn?",
      "CNNs Match ViTs at Scale",
      "DeiT",
      "Swin Transformer",
      "CvT",
      "LeViT",
      "BEiT",
      "MobileViT",
      "Masked AutoEncoder",
      "Max ViT",
      "Swin Transformer v2",
      "EfficientFormer",
      "FastVit",
      "Efficient ViT",
      "SoViT",
    ],
  },
  {
    title: "Low Rank Adaptors",
    link: "low-rank-adaptors",
    papers: [
      "LoRA",
      "DyLoRA",
      "AdaLoRA",
      "QLoRA",
      "LoRA-FA",
      "Delta-LoRA",
      "LongLoRA",
      "VeRA",
      "LoRA+",
      "MoRA",
      "DoRA",
    ],
  },
];

const literature_review_data = [
  {
    title: "Convolutional Neural Networks",
    link: "https://ritvik19.medium.com/papers-explained-review-01-convolutional-neural-networks-78aeff61dcb3",
    papers: [
      "LeNet",
      "AlexNet",
      "VGG",
      "Inception Net",
      "ResNet",
      "Inception Net v2 / v3",
      "ResNext",
      "DenseNet",
      "Xception",
      "MobileNet V1",
      "MobileNet V2",
      "MobileNet V3",
      "EfficientNet",
    ],
  },
  {
    title: "Layout Transformers",
    link: "https://ritvik19.medium.com/papers-explained-review-02-layout-transformers-b2d165c94ad5",
    papers: [
      "Layout LM",
      "LamBERT",
      "Layout LM v2",
      "Structural LM",
      "Doc Former",
      "BROS",
      "LiLT",
      "Layout LM V3",
      "ERNIE Layout",
    ],
  },
  {
    title: "Region Based Convolutional Neural Networks",
    link: "https://ritvik19.medium.com/papers-explained-review-03-rcnns-42c0a3974493",
    papers: ["RCNN", "Fast RCNN", "Faster RCNN", "Mask RCNN", "Cascade RCNN"],
  },
  {
    title: "Tabular Deep Learning",
    link: "https://ritvik19.medium.com/papers-explained-review-04-tabular-deep-learning-776db04f965b",
    papers: [
      "Entity Embeddings",
      "Wide and Deep Learning",
      "Deep and Cross Network",
      "Tab Transformer",
      "Tabular ResNet",
      "Feature Tokenizer Transformer",
    ],
  },
  {
    title: "Generative Adversarial Networks",
    link: "https://ritvik19.medium.com/papers-explained-review-05-generative-adversarial-networks-bbb51b160d5e",
    papers: [
      "GAN",
      "Conditional GAN",
      "Deep Convolutional GAN",
      "Improved GAN",
      "Wasserstein GAN",
      "Cycle GAN",
    ],
  },
  {
    title: "Parameter Efficient FineTuning",
    link: "https://ritvik19.medium.com/papers-explained-review-06-parameter-efficient-finetuning-6934fafa74e5",
    papers: [
      "LoRA",
      "DyLoRA",
      "AdaLoRA",
      "QLoRA",
      "LoRA-FA",
      "Delta-LoRA",
      "LongLoRA",
      "VeRA",
      "LoRA+",
      "MoRA",
      "DoRA",
    ],
  },
  {
    title: "Convolution Layers",
    link: "https://ritvik19.medium.com/papers-explained-review-07-convolution-layers-c083e7410cd3",
    papers: [
      "Convolution Layer",
      "Separable Convolution",
      "Pointwise Convolution",
      "Depthwise Convolution",
      "Convolution Transpose",
    ],
  },
  {
    title: "Recurrent Layers",
    link: "https://ritvik19.medium.com/papers-explained-review-08-recurrent-layers-ff2f224af059",
    papers: ["Simple Recurrent", "LSTM", "GRU"],
  },
  {
    title: "Attention Layers",
    link: "https://ritvik19.medium.com/papers-explained-review-09-attention-layers-beeef323e7f5",
    papers: [
      "Scaled Dot Product Attention",
      "Multi Head Attention",
      "Cross Attention",
      "Causal Attention",
      "Sliding Window Attention",
      "Multi Query Attention",
      "Grouped Query Attention",
    ],
  },
  {
    title: "Normalization Layers",
    link: "https://ritvik19.medium.com/papers-explained-review-10-normalization-layers-56b556c9646e",
    papers: [
      "Batch Normalisation",
      "Layer Normalisation",
      "Instance Normalisation",
      "Group Normalisation",
      "Weight Standardisation",
      "Batch Channel Normalisation",
    ],
  },
  {
    title: "Auto Encoders",
    link: "https://ritvik19.medium.com/papers-explained-review-11-auto-encoders-3b8f08b4eac0",
    papers: [
      "Auto Encoders",
      "Sparse Auto Encoders",
      "K Sparse Auto Encoders",
      "Contractive Auto Encoders",
      "Convolutional Auto Encoders",
      "Sequence to Sequence Auto Encoders",
      "Denoising Auto Encoders",
      "Variational Auto Encoders",
      "Masked Auto Encoders",
    ],
  },
  {
    title: "LLMs for Maths",
    link: "https://ritvik19.medium.com/papers-explained-review-12-llms-for-maths-1597e3c7251c",
    papers: [
      "Wizard Math",
      "MAmmoTH",
      "MetaMath",
      "ToRA",
      "Math Coder",
      "MuggleMath",
      "Llemma",
      "MuMath",
      "MMIQC",
      "DeepSeek Math",
      "Open Math Instruct 1",
      "Math Orca",
      "Math Genie",
      "Xwin-Math",
      "MuMath Code",
      "Numina Math",
      "Qwen 2 Math",
      "Qwen 2.5 Math",
      "Open Math Instruct 2",
      "Math Coder 2",
      "AceMath",
    ],
  },
  {
    title: "Model Merging",
    link: "https://ritvik19.medium.com/papers-explained-review-13-model-merging-d0db49797b90",
    papers: [
      "Model Soup",
      "Spherical Linear Interpolation (SLERP)",
      "Nearswap",
      "Task Arithmetic",
      "Trim, Elect Sign & Merge (TIES)",
      "Drop And REscale (DARE)",
      "Model Breadcrumbs",
      "Model Stock",
      "NuSLERP (Normalized SLERP)",
      "Drop and rEscaLe via sampLing with mAgnitude (DELLA)",
      "Select, Calculate, and Erase (SCE)",
    ]
  }
];