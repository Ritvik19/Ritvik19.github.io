const datasets = [
  {
    title: "Obelics",
    link: "https://ritvik19.medium.com/papers-explained-179-obelics-idefics-a581f8d909b6",
    date: "June 2023",
    description:
      "An open web-scale filtered dataset of interleaved image-text documents comprising 141M web pages, 353M associated images, and 115B text tokens, extracted from CommonCrawl.",
    tags: ["Datasets", "Multimodal Datasets", "HuggingFace"],
  },
  {
    title: "Dolma",
    link: "https://ritvik19.medium.com/papers-explained-97-dolma-a656169269cb",
    date: "January 2024",
    description:
      "An open corpus of three trillion tokens designed to support language model pretraining research.",
    tags: ["Datasets", "Language Model Datasets", "Olmo"],
  },
  {
    title: "Aya Dataset",
    link: "https://ritvik19.medium.com/papers-explained-108-aya-dataset-9e299ac74a19",
    date: "February 2024",
    description:
      "A human-curated instruction-following dataset that spans 65 languages, created to bridge the language gap in datasets for natural language processing.",
    tags: ["Datasets", "Multilingual Datasets", "Cohere"],
  },
  {
    title: "WebSight",
    link: "https://ritvik19.medium.com/papers-explained-177-websight-2905d0e14233",
    date: "March 2024",
    description:
      "A synthetic dataset consisting of 2M pairs of HTML codes and their corresponding screenshots, generated through LLMs, aimed to accelerate research for converting a screenshot into a corresponding HTML. ",
    tags: ["Datasets", "Multimodal Datasets", "HuggingFace"],
  },
  {
    title: "Cosmopedia",
    link: "https://ritvik19.medium.com/papers-explained-175-cosmopedia-5f7e81c76d14",
    date: "March 2024",
    description:
      "Synthetic Data containing over 30M files and 25B tokens, generated by Mixtral-8x7B-Instruct-v0., aimed to reproduce the training data for Phi-1.5.",
    tags: [
      "Datasets",
      "Language Model Datasets",
      "HuggingFace",
      "Synthetic Data",
    ],
  },
  {
    title: "RewardBench",
    link: "A benchmark dataset and code-base designed to evaluate reward models used in RLHF",
    date: "March 2024",
    description:
      "A benchmark dataset and code-base designed to evaluate reward models used in RLHF.",
    tags: ["Datasets", "LLM Evaluation"],
  },
  {
    title: "Fine Web",
    link: "https://ritvik19.medium.com/papers-explained-174-fineweb-280bbc08068b",
    date: "May 2024",
    description:
      "A large-scale dataset for pretraining LLMs, consisting of 15T tokens, shown to produce better-performing models than other open pretraining datasets.",
    tags: ["Datasets", "Language Model Datasets", "HuggingFace"],
  },
  {
    title: "Numina Math",
    link: "https://ritvik19.medium.com/paper-explained-316-numinamath-40501ae9baac",
    date: "July 2024",
    description:
      "A public AI4Maths dataset, comprising 860,000 competition math problems and solutions, ranging from high-school to advanced competition levels, annotated with chain-of-thought traces. It aims to improve mathematical reasoning in LLMs and is instrumental in developing a model that won the 1st AIMO Progress Prize, demonstrating its effectiveness in advancing state-of-the-art mathematical reasoning models.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "Cosmopedia v2",
    link: "https://ritvik19.medium.com/papers-explained-175-cosmopedia-5f7e81c76d14#5bab",
    date: "July 2024",
    description:
      "An enhanced version of Cosmopedia, with a lot of emphasis on prompt optimization.",
    tags: [
      "Datasets",
      "Language Model Datasets",
      "HuggingFace",
      "Synthetic Data",
    ],
  },
  {
    title: "Docmatix",
    link: "https://ritvik19.medium.com/papers-explained-178-docmatix-9f2731ff1654",
    date: "July 2024",
    description:
      "A massive dataset for DocVQA containing 2.4M images, 9.5M question-answer pairs, and 1.3M PDF documents, generated by taking transcriptions from the PDFA OCR dataset and using a Phi-3-small model to generate Q/A pairs. ",
    tags: ["Datasets", "Document Understanding", "HuggingFace"],
  },
  {
    title: "PixMo",
    link: "https://ritvik19.medium.com/papers-explained-241-pixmo-and-molmo-239d70abebff",
    date: "September 2024",
    description:
      "A high-quality dataset of detailed image descriptions collected through speech-based annotations, enabling the creation of more robust and accurate VLMs.",
    tags: ["Datasets", "Multimodal Datasets"],
  },
  {
    title: "OmniMath",
    link: "https://ritvik19.medium.com/papers-explained-364-omnimath-bd2687c23e81",
    date: "October 2024",
    description:
      "A comprehensive benchmark dataset designed to evaluate the mathematical reasoning abilities of LLMs at the Olympiad level, comprising 4428 competition-level problems across 33 sub-domains and 10 difficulty levels, with a rigorous evaluation process utilizing GPT-4o and an open-source verifier, OmniJudge, to address the limitations of existing benchmarks that are now easily solved by advanced LLMs.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "Smol Talk",
    link: "https://ritvik19.medium.com/papers-explained-176-smol-lm-a166d5f1facc#b5e3",
    date: "November 2024",
    description:
      "A synthetic instruction-following dataset comprising 1 million samples, built using a fine-tuned LLM on a diverse range of instruction-following datasets and then generating synthetic conversations using various prompts and instructions to improve instruction following, chat, and reasoning capabilities.",
    tags: ["Datasets", "Synthetic Data", "HuggingFace"],
  },
  {
    title: "Red Pajama V1",
    link: "https://ritvik19.medium.com/papers-explained-299-red-pajama-4aced4a3ff72",
    date: "November 2024",
    description:
      "A reproduction of the LLaMA training dataset, built from seven sources (CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange) totaling 1.2 trillion tokens. The reproduction process involved addressing gaps and ambiguities in the original LLaMA documentation, with some differences in data processing choices.",
    tags: ["Datasets"],
  },
  {
    title: "Red Pajama V2",
    link: "https://ritvik19.medium.com/papers-explained-299-red-pajama-4aced4a3ff72#9376",
    date: "November 2024",
    description:
      "A massive, unfiltered web-based dataset derived from CommonCrawl, comprising over 100 trillion tokens in multiple languages. It includes various quality signals (natural language metrics, repetitiveness, content flags, ML heuristics, and deduplication data) as metadata, enabling flexible filtering and dataset creation for diverse downstream tasks.",
    tags: ["Datasets"],
  },
  {
    title: "MVTamperBench",
    link: "https://ritvik19.medium.com/papers-explained-402-mvtamperbench-828a22e9e0b9",
    date: "December 2024",
    description:
      "A benchmark designed to evaluate the robustness of Multimodal Language Models against five prevalent video tampering techniques: rotation, masking, substitution, repetition, and dropping. It comprises 3.4K original videos expanded into over 17K tampered clips.",
    tags: ["Datasets", "Benchmark", "Multimodal Datasets"],
  },
  {
    title: "Big Math",
    link: "https://ritvik19.medium.com/papers-explained-410-big-math-426eacadc021",
    date: "February 2025",
    description:
      "A dataset of over 250,000 high-quality math questions designed for reinforcement learning (RL) in language models, addressing the gap between data quality and quantity in existing math datasets. It details the rigorous filtering, cleaning, and curation process, including the creation of Big-Math-Reformulated (47,000 reformulated questions).",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "Numina Math 1.5",
    link: "https://ritvik19.medium.com/paper-explained-316-numinamath-40501ae9baac#e825",
    date: "February 2025",
    description:
      "An update over NuminaMath dataset containing approximately 900k competition-level math problems with Chain of Thought (CoT) solutions, sourced from Chinese high school exercises to international mathematics olympiads. It includes metadata like answer, problem_type, and question_type for each problem, and features manually curated data from olympiads, contests, and specific mathematical domains, while removing the synthetic dataset synthetic_amc.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "OpenCodeReasoning",
    link: "https://ritvik19.medium.com/papers-explained-361-opencodereasoning-0e0b4439324a",
    date: "April 2025",
    description:
      "A publicly available dataset containing 736,712 Python code solutions with accompanying reasoning traces, spanning 28,904 unique competitive programming questions generated by DeepSeek-R1, designed to enhance the reasoning capabilities of LLMs in coding tasks through SFT.",
    tags: ["Datasets", "Code Data"],
  },
  {
    title: "DeepMath",
    link: "https://ritvik19.medium.com/papers-explained-365-deepmath-496109878202",
    date: "April 2025",
    description:
      "A large-scale dataset of ~103K challenging math problems designed for training reasoning models via reinforcement learning or supervised finetuning. It features verifiable answers for rule-based RL, three distinct AI-generated solutions per problem for diverse training approaches, and rigorous decontamination against existing benchmarks to ensure evaluation integrity and promote generalizable reasoning.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "SweEval",
    link: "https://ritvik19.medium.com/papers-explained-397-sweeval-f779d7da1196",
    date: "May 2025",
    description:
      "A cross-lingual enterprise safety benchmark designed to evaluate Large Language Models (LLMs) in handling sensitive language across diverse linguistic and cultural contexts. It assesses whether LLMs comply with or resist inappropriate instructions, such as including swear words, and evaluates their alignment with ethical frameworks, cultural nuances.",
    tags: ["Datasets", "Benchmark"],
  },
  {
    title: "OpenThoughts",
    link: "https://ritvik19.medium.com/papers-explained-394-openthoughts-51fcf3dda8d2",
    date: "June 2025",
    description:
      "The OpenThoughts project aims to create open-source datasets for training reasoning models, leading to the development of OpenThinker models.",
    tags: ["Datasets", "Scientific Data", "Open Source"],
  },
  {
    title: "OMEGA",
    link: "https://ritvik19.medium.com/papers-explained-414-out-of-distribution-math-problems-evaluation-with-3-generalization-axes-ac4abe71a794",
    date: "June 2025",
    description:
      "A benchmark to evaluate LLMs' out-of-distribution generalization in math across exploratory, compositional, and transformative axes. Experiments using OMEGA reveal that while LLMs show some improvement in exploratory generalization with fine-tuning, they struggle with compositional and transformative reasoning, highlighting a gap between LLM reasoning and human mathematical creativity.",
    tags: ["Datasets", "Benchmark", "Scientific Data"],
  },
  {
    title: "TabArena",
    link: "https://ritvik19.medium.com/papers-explained-418-tabarena-ff7e5159e982",
    date: "June 2025",
    description:
      "A continuously maintained, living benchmark for tabular machine learning models, featuring a curated collection of 51 datasets, 16 models, focusing on tabular classification and regression for independent and identically distributed (IID) data in the small to medium data regime.",
    tags: ["Datasets", "Benchmark", "Tabular Data"],
  },
  {
    title: "FineWeb 2",
    link: "https://ritvik19.medium.com/papers-explained-459-fineweb2-d9126117600e",
    date: "June 2025",
    description:
      "A 20TB (5B document) multilingual dataset covering over 1000 languages, created using a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. The pipeline includes steps for Language Identification, Deduplication, Filtering, and Dedup-informed upsampling (Rehydration), each of which improves performance.",
    tags: ["Datasets", "HuggingFace"],
  },
  {
    title: "NaturalThoughts",
    link: "https://ritvik19.medium.com/360e8075f17c",
    date: "July 2025",
    description:
      "Curates a high-quality dataset of reasoning traces curated by selecting examples from NaturalReasoning to improve the reasoning capabilities of smaller language models through supervised finetuning. The study demonstrates that scaling high-quality, diverse reasoning data and selecting difficult examples requiring diverse reasoning strategies are more effective for distilling reasoning skills, and that training with a mix of System-1 (final answer only) and System-2 (full reasoning traces) distillation improves inference-time efficiency.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "OpenCodeReasoning II",
    link: "https://ritvik19.medium.com/papers-explained-440-opencodereasoning-ii-c1e27ef6fb5e",
    date: "July 2025",
    description:
      "A large-scale dataset containing 2.5 million question-solution-critique triples for approximately 35,000 unique programming questions, featuring reasoning CoTs for both solutions and critiques. It also includes an extension to the LiveCodeBench benchmark to support C++ programming.",
    tags: ["Datasets", "Code Data"],
  },
  {
    title: "MegaScience",
    link: "https://ritvik19.medium.com/papers-explained-435-megascience-ffe3fe3a8040",
    date: "July 2025",
    description:
      "Introduces TextBookReasoning, a dataset of 650k reasoning questions extracted from 12k university-level science textbooks, and MegaScience, a 1.25 million instance dataset created by combining high-quality open-source datasets.",
    tags: ["Datasets", "Scientific Data"],
  },
  {
    title: "FineVision",
    link: "https://ritvik19.medium.com/e4b1af24ecbf",
    date: "September 2025",
    description:
      "A multimodal dataset with 24 million samples, created by collecting over 200 datasets containing 17M images, 89M question-answer turns, and 10B answer tokens, totaling 5TB of high-quality data.",
    tags: ["Datasets", "Multimodal Datasets", "HuggingFace"],
  },
];
