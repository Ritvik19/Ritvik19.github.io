const llm_training = [
  {
    title: "Self-Taught Reasoner (STaR)",
    link: "https://ritvik19.medium.com/papers-explained-288-star-cf485a5b117e",
    date: "May 2022",
    description:
      "A bootstrapping method that iteratively improves a language model's reasoning abilities by generating rationales for a dataset, filtering for rationales that lead to correct answers, fine-tuning the model on these successful rationales, and repeating this process, optionally augmented by 'rationalization' where the model generates rationales given the correct answer as a hint.",
    tags: [],
  },
  {
    title: "Self Instruct",
    link: "https://ritvik19.medium.com/papers-explained-112-self-instruct-5c192580103a",
    date: "December 2022",
    description:
      "A framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Constitutional AI",
    link: "https://ritvik19.medium.com/papers-explained-411-constitutional-ai-db2e526c6f13",
    date: "December 2022",
    description:
      "A method for training harmless AI assistants through self-improvement, using a list of rules or principles (a constitution) instead of human labels for harmful outputs. The process involves a supervised learning phase (critique, revision, and fine-tuning) and a reinforcement learning phase (AI preference evaluation and RL from AI Feedback), resulting in a non-evasive AI assistant that explains its objections to harmful queries.",
    tags: [],
  },
  {
    title: "Reinforced Self-Training (ReST)",
    link: "https://ritvik19.medium.com/papers-explained-301-rest-6389371a68ac",
    date: "April 2023",
    description:
      "Iteratively improves a language model by generating a dataset of samples from the current policy (Grow step), filtering those samples based on a reward model derived from human preferences (Improve step), and then fine-tuning the model on the filtered data using an offline RL objective, repeating this process with increasing filtering thresholds to continually refine the model's output quality.",
    tags: [],
  },
  {
    title: "Reward rAnked FineTuning (RAFT)",
    link: "https://ritvik19.medium.com/papers-explained-303-reward-ranked-finetuning-raft-791154585908",
    date: "April 2023",
    description:
      "An alignment framework for generative foundation models that iteratively refines the model by sampling outputs, ranking them based on a reward function, and then fine-tuning the model on the highest-ranked samples. This approach offers improved stability and efficiency compared to RLHF methods like PPO, reduces memory burden by decoupling data generation and fine-tuning, and is flexible across both LLMs and diffusion models.",
    tags: [],
  },
  {
    title: "ReST^EM",
    link: "https://ritvik19.medium.com/papers-explained-302-rest-em-9abe7c76936e",
    date: "December 2023",
    description:
      "A self-training method based on expectation-maximization for reinforcement learning with language models. It iteratively generates samples from the model, filters them using binary feedback (E-step), and fine-tunes the base pretrained model on these filtered samples (M-step). Unlike the original ReST, ReST^EM doesn't augment with human data and fine-tunes the base model each iteration, improving transfer performance.",
    tags: [],
  },
  {
    title: "Direct Preference Optimization",
    link: "https://ritvik19.medium.com/papers-explained-148-direct-preference-optimization-d3e031a41be1",
    date: "December 2023",
    description:
      "A stable, performant, and computationally lightweight algorithm that fine-tunes llms to align with human preferences without the need for reinforcement learning, by directly optimizing for the policy best satisfying the preferences with a simple classification objective.",
    tags: [],
  },
  {
    title: "V-STaR",
    link: "https://ritvik19.medium.com/papers-explained-289-v-star-4d2aeedab861",
    date: "February 2024",
    description:
      "Iteratively improves a language model's reasoning abilities by training a verifier with Direct Preference Optimization (DPO) on both correct and incorrect solutions generated by the model, while simultaneously fine-tuning the generator on only the correct solutions, ultimately using the verifier at inference time to select the best solution among multiple candidates.",
    tags: [],
  },
  {
    title: "Retrieval Augmented Fine Tuning (RAFT)",
    link: "https://ritvik19.medium.com/papers-explained-272-raft-5049520bcc26",
    date: "March 2024",
    description:
      "A training method that enhances the performance of LLMs for open-book in-domain question answering by training them to ignore irrelevant documents, cite verbatim relevant passages, and promote logical reasoning.",
    tags: [],
  },
  {
    title: "WRAP",
    link: "https://ritvik19.medium.com/papers-explained-118-wrap-e563e009fe56",
    date: "March 2024",
    description:
      "Uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles to jointly pre-train LLMs on real and synthetic rephrases.",
    tags: ["Synthetic Data"],
  },
  {
    title: "RLHF Workflow",
    link: "https://ritvik19.medium.com/papers-explained-149-rlhf-workflow-56b4e00019ed",
    date: "May 2024",
    description:
      "Provides a detailed recipe for  online iterative RLHF and achieves state-of-the-art performance on various benchmarks using fully open-source datasets.",
    tags: [],
  },
  {
    title: "Magpie",
    link: "https://ritvik19.medium.com/papers-explained-183-magpie-0603cbdc69c3",
    date: "June 2024",
    description:
      "A self-synthesis method that extracts high-quality instruction data at scale by prompting an aligned LLM with left-side templates, generating 4M instructions and their corresponding responses.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Instruction Pre-Training",
    link: "https://ritvik19.medium.com/papers-explained-184-instruction-pretraining-ee0466f0fd33",
    date: "June 2024",
    description:
      "A framework to augment massive raw corpora with instruction-response pairs enabling supervised multitask pretraining of LMs.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Persona Hub",
    link: "https://ritvik19.medium.com/papers-explained-321-persona-hub-a1b496192a4c",
    date: "June 2024",
    description:
      "A persona-driven method for creating diverse synthetic data using LLMs, introducing Persona Hub, a collection of 1 billion automatically curated personas from web data. These personas act as diverse knowledge carriers, enabling the LLM to generate varied synthetic data (e.g., math problems, text, game NPCs) at scale.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Self-Taught Evaluators",
    link: "https://ritvik19.medium.com/papers-explained-276-self-taught-evaluators-8270905392ed",
    date: "August 2024",
    description:
      "An iterative training scheme that uses only synthetically generated preference data, without human annotations, to improve an LLM's ability to judge the quality of model responses by iteratively generating contrasting model outputs, training an LLM-as-a-Judge to produce reasoning traces and judgments, and using the improved predictions in subsequent iterations.",
    tags: [],
  },
  {
    title: "Direct Judgement Preference Optimization",
    link: "https://ritvik19.medium.com/papers-explained-228-direct-judgement-preference-optimization-6915425402bf",
    date: "September 2024",
    description:
      "Proposes learning through preference optimization to enhance the evaluation capabilities of LLM judges which are trained on three approaches: Chain-of-Thought Critique, Standard Judgement, and Response Deduction across various use cases, including single rating, pairwise comparison, and classification.",
    tags: ["LLM Evaluation"],
  },
  {
    title: "Constrained Generative Policy Optimization (Mixture of Judges)",
    link: "https://ritvik19.medium.com/papers-explained-304-constrained-generative-policy-optimization-mixture-of-judges-71ae4b508b74",
    date: "September 2024",
    description:
      "An LLM post-training paradigm using a Mixture of Judges (MoJ) and cost-efficient constrained policy optimization with stratification to address reward hacking and multi-objective optimization challenges in RLHF. It achieves this by employing rule-based and LLM-based judges to identify and constrain undesirable generation patterns while maximizing calibrated rewards, using tailored optimization strategies for each task in a multi-task setting to avoid conflicting objectives and improve the Pareto frontier.",
    tags: [],
  },
  {
    title: "LongCite",
    link: "https://ritvik19.medium.com/papers-explained-273-longcite-4800340e51d7",
    date: "October 2024",
    description:
      "A system comprising LongBench-Cite benchmark, CoF pipeline for generating cited QA instances, LongCite-45k dataset, and LongCite-8B/9B models trained on this dataset to improve the trustworthiness of long-context LLMs by enabling them to generate responses with fine-grained sentence-level citations.",
    tags: [],
  },
  {
    title: "Thought Preference Optimization",
    link: "https://ritvik19.medium.com/papers-explained-274-thought-preference-optimization-4f365380ae74",
    date: "October 2024",
    description:
      "Iteratively trains LLMs to generate useful 'thoughts' that improve response quality by prompting the model to produce thought-response pairs, scoring the responses with a judge model, creating preference pairs from the highest and lowest-scoring responses and their associated thoughts, and then using these pairs with DPO or IRPO loss to optimize the thought generation process while mitigating judge model length bias through score normalization.",
    tags: [],
  },
  {
    title: "Self-Consistency Preference Optimization",
    link: "https://ritvik19.medium.com/papers-explained-275-self-consistency-preference-optimization-ccd08f5acafb",
    date: "November 2024",
    description:
      "An unsupervised iterative training method for LLMs that leverages the concept of self-consistency to create preference pairs by selecting the most consistent response as the chosen response and the least consistent one as the rejected response, and then optimizes a weighted loss function that prioritizes pairs with larger vote margins, reflecting the model's confidence in the preference.",
    tags: [],
  },
  {
    title: "Hyperfitting",
    link: "https://ritvik19.medium.com/papers-explained-305-hyperfitting-8d9ad63797cd",
    date: "December 2024",
    description:
      "Involves fine-tuning a pre-trained LLM on a small dataset until near-zero training loss, significantly improving greedy decoding generation quality despite worsening validation loss. This counter-intuitive process sharpens the model's prediction space, often favoring single tokens, and enhances long-sequence generation even with citation blocking, suggesting the improvement isn't simply memorization.",
    tags: [],
  },
  {
    title: "rStar-Math",
    link: "https://ritvik19.medium.com/papers-explained-290-rstar-math-4b3317a2c2c6",
    date: "January 2025",
    description:
      "Uses a deep thinking approach with Monte Carlo Tree Search and smaller language models to achieve state-of-the-art math reasoning, rivaling or surpassing larger models like OpenAI's. It employs a novel code-augmented CoT data synthesis, a process preference model (PPM) trained with pairwise ranking, and a self-evolution recipe to iteratively improve SLM performance on complex math problems, including Olympiad-level questions.",
    tags: ["LLM for Math"],
  },
  {
    title: "Multiagent Finetuning",
    link: "https://ritvik19.medium.com/papers-explained-292-multiagent-finetuning-a199fc4d8446",
    date: "January 2025",
    description:
      "Improves large language models by training a 'society' of specialized models (generation and critic agents) on data generated through multiagent debate. Generation agents are fine-tuned on their own correct initial responses, while critic agents are fine-tuned on debate sequences showing both initial incorrect and final corrected answers, fostering diversity and enabling iterative self-improvement over multiple rounds.",
    tags: [],
  },
  {
    title: "Critique Fine-Tuning",
    link: "https://ritvik19.medium.com/papers-explained-306-critique-fine-tuning-738b25d1b51c",
    date: "January 2025",
    description:
      "Trains language models to critique noisy responses to questions, rather than simply imitating correct answers, leading to deeper understanding and improved reasoning.",
    tags: [],
  },
  {
    title: "Diverse Preference Optimization",
    link: "https://ritvik19.medium.com/papers-explained-307-diverse-preference-optimization-7f99326e264c",
    date: "January 2025",
    description:
      "Enhances response diversity in language models by selecting preference pairs based on both reward and a diversity criterion. Instead of contrasting the highest and lowest rewarded responses, DivPO contrasts the most diverse response above a reward threshold with the least diverse response below the threshold, promoting a wider range of high-quality outputs.",
    tags: [],
  },
  {
    title: "SFT Memorizes, RL Generalizes",
    link: "https://ritvik19.medium.com/papers-explained-308-sft-memorizes-rl-generalizes-f51c5c66ea05",
    date: "January 2025",
    description:
      "Investigates the comparative effects of SFT and RL on foundation model generalization and memorization in text and visual tasks (GeneralPoints and V-IRL), finding that RL significantly improves generalization in both rule-based and visual out-of-distribution scenarios while SFT primarily memorizes training data; SFT stabilizes output format for subsequent RL gains, and scaling inference-time compute (verification steps) further improves RL generalization.",
    tags: [],
  },
  {
    title: "SelfCite",
    link: "https://ritvik19.medium.com/papers-explained-selfcite-841a9e60006a",
    date: "February 2025",
    description:
      "A self-supervised method for improving the quality of citations generated by LLMs. It uses context ablation to create a reward signal based on the necessity and sufficiency of cited text, guiding best-of-N sampling or preference optimization (like SimPO) to train LLMs to generate better citations without relying on human annotations.",
    tags: [],
  },
  {
    title: "Selective Self-to-Supervised Fine-Tuning (S3FT)",
    link: "https://ritvik19.medium.com/papers-explained-325-selective-self-to-supervised-fine-tuning-s3ft-b2602400d938",
    date: "February 2025",
    description:
      "Improves upon standard SFT by leveraging the existence of multiple valid responses to a given input. It selectively fine-tunes an LLM on its own correct predictions when they align with the gold response, and on gold responses (or paraphrased versions) otherwise, mitigating overfitting and improving generalization performance.",
    tags: [],
  },
  {
    title: "SysGen",
    link: "https://ritvik19.medium.com/papers-explained-323-sysgen-e0826a205925",
    date: "February 2025",
    description:
      "A pipeline for generating system messages and corresponding aligned assistant responses for existing SFT datasets that lack system messages. It annotates phrases within system messages with eight key functionalities, filters erroneous tags, verifies functionality appropriateness using an LLM-as-a-judge approach, and then generates new, better-aligned assistant responses based on the refined system messages and original user instructions.",
    tags: [],
  },
  {
    title: "Thinking Preference Optimization (ThinkPO)",
    link: "https://ritvik19.medium.com/papers-explained-324-thinking-preference-optimization-d3ab029c77b5",
    date: "February 2025",
    description:
      "A post SFT method that improves long CoT reasoning in LLMs by leveraging existing SFT data and readily available short chain-of-thought responses using DPO, treating long reasoning responses as preferred, encouraging the model to generate longer, more reasoned outputs without needing new, costly long CoT data.",
    tags: [],
  },
  {
    title: "Code Guided Synthetic data generation system (CoSyn)",
    link: "https://ritvik19.medium.com/papers-explained-339-code-guided-synthetic-data-generation-system-cosyn-22b7f371906b",
    date: "February 2025",
    description:
      "A framework leveraging LLMs to generate synthetic text-rich multimodal data for training (VLMs. It uses LLMs to create code in various languages (Python, HTML, LaTeX, etc.) that renders synthetic images, and then uses the code as context to generate corresponding textual instructions, including questions, answers, and explanations, forming a comprehensive instruction-tuning dataset.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Logic-RL",
    link: "https://ritvik19.medium.com/papers-explained-337-logic-rl-6f1ae1ffaf09",
    date: "February 2025",
    description:
      "A rule-based reinforcement learning framework trained on procedurally generated Knights and Knaves logic puzzles to enhance reasoning skills in large language models. It utilizes a modified REINFORCE++ algorithm with a strict format and answer-based reward system, enabling the model to develop advanced reasoning capabilities like reflection and verification, and generalize to challenging math benchmarks like AIME and AMC after training on a small dataset.",
    tags: [],
  },
  {
    title: "SPHERE",
    link: "https://ritvik19.medium.com/papers-explained-380-self-evolved-preference-optimization-sphere-f256c1a7bb0f",
    date: "March 2025",
    description:
      "A self-evolving data generation pipeline that enhances mathematical reasoning in SLMs by iteratively generating, correcting, and diversifying reasoning chains through three stages: self-generation of initial reasoning paths, self-correction of errors in those paths, and diversity generation by introducing varied incorrect reasoning using a smaller model; this data is then used to fine-tune the SLM with Direct Preference Optimization.",
    tags: ["LLM for Math"],
  },
  {
    title: "AdaptiVocab",
    link: "https://ritvik19.medium.com/papers-explained-421-adaptivocab-5861615452bc",
    date: "March 2025",
    description:
      "An end-to-end approach for vocabulary adaptation designed to enhance LLM efficiency in low-resource domains by adapting the vocabulary to focused domains of interest. It modifies the vocabulary by replacing tokens with domain-specific n-gram-based tokens, reducing the number of tokens required for input processing and output generation, and initializes new n-token embeddings using an exponentially weighted combination of existing embeddings, followed by a lightweight fine-tuning phase.",
    tags: [],
  },
  {
    title: "ReSearch",
    link: "https://ritvik19.medium.com/papers-explained-349-research-80c79cb22fed",
    date: "March 2025",
    description:
      "A framework that trains LLMs to reason with search via reinforcement learning, treating search operations as integral components of the reasoning chain, leveraging GRPO to optimize LLMs for generating reasoning chains containing text-based thinking, search queries, and retrieval results.",
    tags: [],
  },
  {
    title: "CLIMB",
    link: "https://ritvik19.medium.com/papers-explained-356-climb-875e43e8357e",
    date: "April 2025",
    description:
      "A clustering-based iterative data mixture bootstrapping framework, automatically discovers optimal data mixtures for language model pre-training by embedding and clustering large datasets, iteratively searching for optimal mixtures using a proxy model and predictor, and refining the mixture through a bootstrapping strategy.",
    tags: [],
  },
  {
    title: "Test Time Reinforcement Learning",
    link: "https://ritvik19.medium.com/papers-explained-370-test-time-reinforcement-learning-ttrl-48416da31110",
    date: "April 2025",
    description:
      "A method for training LLMs on unlabeled data using reinforcement learning, where reward signals are generated by comparing model outputs from repeated sampling with a consensus output (e.g., derived from majority voting) acting as a proxy for the optimal action, enabling self-evolution of the model during inference without ground truth labels.",
    tags: [],
  },
  {
    title: "One-Shot RLVR",
    link: "https://ritvik19.medium.com/papers-explained-373-one-shot-rlvr-2258fffd857f",
    date: "April 2025",
    description:
      "Trains an LLM using reinforcement learning with a verifiable reward (e.g., a binary correct/incorrect signal) on only a single training example, surprisingly achieving comparable performance to training on much larger datasets in mathematical reasoning tasks.",
    tags: ["LLM for Math"],
  },
  {
    title: "Absolute Zero",
    link: "https://ritvik19.medium.com/papers-explained-375-absolute-zero-dc2e175488c3",
    date: "May 2025",
    description:
      "A new reinforcement learning paradigm that trains reasoning models without any human-curated data. It involves a single model learning to propose tasks that maximize its own learning progress and improving reasoning by solving them, using a code executor to validate tasks and verify answers, enabling continuous self-improvement without human intervention.",
    tags: [],
  },
  {
    title: "REFINE-AF",
    link: "https://ritvik19.medium.com/papers-explained-376-refine-af-7879070779b2",
    date: "May 2025",
    description:
      "A semi-automated framework designed to generate high-quality instruction-input-output triplets for new tasks, using Reinforcement Learning from Automated Feedback. It utilizes a small seed set of manually written tasks to generate instructions, employs reinforcement learning to improve the quality of input-output pairs, and constructs an Instruction Fine Tuning dataset for refining base models through supervised fine-tuning.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Perplexity-based Importance Refinement (PIR)",
    link: "https://ritvik19.medium.com/papers-explained-390-perplexity-based-importance-refinement-pir-68103a8da269",
    date: "May 2025",
    description:
      "A framework that optimizes chain-of-thought data by identifying and pruning low-importance functional steps in reasoning chains, while preserving the core progressive reasoning. Fine-tuning models on PIR-optimized data results in improved accuracy and reduced token usage across challenging reasoning benchmarks.",
    tags: ["Language Models", "LLM for Math"],
  },
  {
    title: "rStar-Coder",
    link: "https://ritvik19.medium.com/papers-explained-396-rstar-coder-eeff4bb0b518",
    date: "May 2025",
    description:
      "A novel approach for improving code reasoning in LLMs by constructing a large-scale, verified dataset of 418K competition-level code problems with 580K long-reasoning solutions and rich test cases. It curates competitive programming problems, synthesizes new solvable problems, generates reliable input-output test cases using a three-step input generation method and mutual verification mechanism, and augments problems with test-case-verified long-reasoning solutions.",
    tags: ["LLM for Code"],
  },
  {
    title: "Reinforcement Learning with Reference Probability Reward (RLPR)",
    link: "https://ritvik19.medium.com/papers-explained-413-reinforcement-learning-with-reference-probability-reward-rlpr-ac742c006a22",
    date: "June 2025",
    description:
      "A framework that extends RLVR to general domains without external verifiers by using the LLM's intrinsic probability of generating a correct answer as the reward signal. It introduces a probability-based reward calculated by the average decoding probabilities of the reference answer tokens, a debiasing method to eliminate reward bias, and an adaptive curriculum learning mechanism to stabilize training by filtering prompts with low reward standard deviation.",
    tags: ["Reinforcement Learning"],
  },
  {
    title: "LongWriter-Zero",
    link: "https://ritvik19.medium.com/papers-explained-416-longwriter-zero-326f86fa5be5",
    date: "June 2025",
    description:
      "A novel approach to ultra-long text generation that uses reinforcement learning to train LLMs, without relying on synthetic data or SFT. By using specialized reward models to guide the LLM towards improved length control, writing quality, and structural formatting.",
    tags: ["Reinforcement Learning"],
  },
  {
    title: "Unary Feedback as Observation",
    link: "https://ritvik19.medium.com/papers-explained-446-unary-feedback-as-observation-ef72b6bd3458",
    date: "July 2025",
    description:
      "Addresses the issue of Large Reasoning Models trained with single-turn Reinforcement Learning struggling with multi-turn problem-solving due to repetitive responses and failure to incorporate feedback, using unary feedback (e.g., \"Let's try again\") in multi-turn RL to improve both single-turn performance and multi-turn reasoning accuracy, along with reward structures to minimize turns and encourage diverse reasoning.",
    tags: ["Reinforcement Learning", "LLM for Math"],
  },
  {
    title: "CoT-Self-Instruct",
    link: "https://ritvik19.medium.com/papers-explained-436-cot-self-instruct-7c95400ef23c",
    date: "July 2025",
    description:
      "A synthetic data generation method that uses LLMs to reason and plan via CoT based on seed tasks, generating new synthetic prompts of similar quality and complexity, then filters the generated data for high quality using automatic metrics like Answer-Consistency for verifiable reasoning tasks and RIP for non-verifiable tasks.",
    tags: ["Synthetic Data"],
  },
  {
    title: "Reinforcement Learning with Calibration Rewards (RLCR)",
    link: "https://ritvik19.medium.com/papers-explained-439-reinforcement-learning-with-calibration-rewards-rlcr-bafda59538fd",
    date: "July 2025",
    description:
      "A training approach for language models that aims to improve both accuracy and calibrated confidence estimation by optimizing a reward function that combines a binary correctness score with a Brier score, incentivizing calibrated prediction.",
    tags: ["Reinforcement Learning"],
  },
  {
    title: "Safe-Completions",
    link: "https://ritvik19.medium.com/papers-explained-430-safe-completions-a8e8176bcc9b",
    date: "August 2025",
    description:
      "A safety-training approachused for GPT-5, which focuses on the safety of the model's output rather than a binary classification of user intent. Safe-completions aim to maximize helpfulness within safety policy constraints, improving safety on dual-use prompts, reducing the severity of safety failures, and increasing model helpfulness compared to traditional refusal-based training.",
    tags: ["Safety"],
  },
  {
    title: "Agent Foundation Models (Chain-of-Agents)",
    link: "https://ritvik19.medium.com/725db27dc0e5",
    date: "August 2025",
    description:
      "A novel LLM reasoning paradigm that enables end-to-end complex problem-solving by simulating multi-agent collaboration within a single model, eliminating the need for complex prompt and workflow engineering. Agent Foundation Models (AFMs) are trained using multi-agent distillation and reinforcement learning.",
    tags: [],
  },
  {
    title: "AggLM",
    link: "https://ritvik19.medium.com/d3ee0e491a98",
    date: "September 2025",
    description:
      "Proposes a method that learns to aggregate multiple solutions generated by an LLM into a final, correct answer using reinforcement learning from verifiable rewards. It involves sampling multiple solutions from an LLM, passing them back to an LLM with an aggregation instruction to synthesize a final answer by reconciling, correcting, and combining the intermediate reasoning steps, and training a reasoning model to perform this aggregation.",
    tags: [],
  },
  {
    title: "LLM-JEPA",
    link: "https://ritvik19.medium.com/ceedfd0e63d8",
    date: "September 2025",
    description:
      "Introduces JEPA-based training objective for LLMs that operates in embedding space using different views of data, such as text and code, to improve representation quality while maintaining generative capabilities.",
    tags: [],
  }
];
