---
markmap:
  color: "#2980b9"
  maxWidth: 800
  initialExpandLevel: 3
---

# [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011)

## &nbsp;
### General techniques used for optimizing small language models, categorized by type of model optimization
and most central constraints they address.
| Technique | General Mechanism | Training Compute | Dataset Size | Inference Runtime | Memory | Storage Space | Latency |
|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **Model Architectures** | | | | | | | |
| | Lightweight Models | âœ“ | | âœ“ | âœ“ | | âœ“ |
| | Efficient Self-Attention | âœ“ | | âœ“ | âœ“ | | âœ“ |
| | Neural Arch. Search | | | âœ“ | âœ“ | âœ“ | |
| **Training Techniques** | | | | | | | |
| | Pre-training | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | |
| | Finetuning | âœ“ | âœ“ | | | | |
| **Model Compression**  | | | | | | | |
| | Pruning | | | âœ“ | âœ“ | âœ“ | âœ“ |
| | Quantization | | | âœ“ | âœ“ | âœ“ | âœ“ |
| | Knowledge Distillation | | âœ“ | | | | |

## Model Architectures

### Lightweight Architectures
* [BabyLLaMA](https://arxiv.org/abs/2308.02019) and [BabyLLaMA-2](https://arxiv.org/abs/2409.17312) distill knowledge from multiple teachers into a 58M and a 345M model respectively, demonstrating that distillation can exceed teacher modelsâ€™ performance particularly under data-constrained conditions.
* [TinyLLaMA](https://arxiv.org/abs/2401.02385) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-93-tinyllama-6ef140170da9) uses FlashAttention (Dao et al., 2022) to optimize memory overhead while maintaining competitive performance for various downstream tasks.
* [MobilLLaMA](https://arxiv.org/abs/2402.16840) applies a parameter-sharing scheme that reduces both pretraining and deployment costs.
* [Mobile LLM](https://arxiv.org/abs/2402.14905) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-216-mobilellm-2d7fdd5acd86)introduces embedding-sharing and grouped-query attention mechanisms with block-wise weight sharing to reduce latency.

### Efficient Self-Attention Approximations
* [Reformer](https://arxiv.org/abs/2001.04451) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-165-reformer-4445ad305191) Improves self-attention complexity from O(N^2) to O(N log N) by replacing dot product attention with locality-sensitivity hashing.
* [Routing Transformer](https://arxiv.org/abs/2003.05997) Uses a Sparse Routing Module based on online k-means clustering to reduce the complexity of the attention computation.
* [Linformer](https://arxiv.org/abs/2006.04768) Expresses self-attention as a linear dot-product of kernel feature maps, reducing quadratic complexity.
* [Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236) Expresses self-attention as a linear dot-product of kernel feature maps, reducing quadratic complexity and viewing transformers as recurrent neural networks for faster inference.
* [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902) Uses the Nystrom method to approximate the self-attention operation with strong empirical performance.
* [Mamba](https://arxiv.org/abs/2312.00752) introduces a selective state space model with input-dependent transitions for linear time and space complexity.
* [RWKV](https://arxiv.org/abs/2305.13048) combines elements of transformers and RNNs with a linear attention mechanism for linear time and space complexity.
* [Longformer](https://arxiv.org/abs/2004.05150) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-38-longformer-9a08416c532e) Uses a combination of local windowed attention and task-specific global attention, scaling linearly with input length for memory efficiency.

### Neural Architecture Search Techniques
* [Mobile LLM](https://arxiv.org/abs/2402.14905) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-216-mobilellm-2d7fdd5acd86) investigates the impact of model depth (i.e., number of layers) and width (i.e., number of heads) on performance, effectively conducting a targeted architecture search within a smaller parameter range.
* [Search for Efficient Large Language Models](https://arxiv.org/abs/2409.17372) reduce the search space by exploring an appropriate initialization for the search.

### Small Multi-modal Models
* [InternVL2](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/) Utilizes outputs from intermediate layers of large visual encoders instead of relying solely on the final outputs. This reduces the size and complexity of the visual encoder while maintaining performance.
* [PaliGemma](https://arxiv.org/abs/2407.07726) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-197-pali-gemma-6899e871998e) Employs smaller and more efficient vision encoder architectures compared to traditional large-scale encoders. This reduces the computational cost and memory footprint of the model.
* Monolithic Multi-Modal Models completely eliminates the separate visual encoder and instead uses lightweight architectures to generate visual tokens directly.
* [Chameleon](https://arxiv.org/abs/2405.09818) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-143-chameleon-6cddfdbceaa8) Employs a VQ-VAE model to encode and decode images into discrete tokens.
* [Mono-InternVL](https://arxiv.org/abs/2410.08202)  Uses an MLP to generate visual tokens for image patches and incorporates a modality-specific feedforward network (multi-modal Mixture-of-Experts) to differentiate between modalities.

## Training Techniques
### Pre-training Techniques

#### Mixed Precision Training Techniques
* Automatic Mixed Precision (AMP):
    * Keeps a master copy of weights in 32-bit floating-point (FP32) precision.
    * Performs arithmetic operations in 16-bit floating-point (FP16) precision.
* Brain Floating Point (BFLOAT16):
    * Offers a greater dynamic range with more exponent bits than FP16.
    * Demonstrates superior training performance and representation accuracy compared to FP16.
* 8-bit Floating-Point (FP8):
    * Supported by NVIDIA's latest Hopper architecture.
    * Enables even greater computational efficiency for large-scale language models.

#### Optimization and Stability Techniques
* Adam and AdamW Optimizers: Commonly used optimizers for training language models.
* Memory-Efficient Optimizers: Adafactor and Sophia improve training speed and efficiency.
* Gradient Clipping: Prevents exploding gradients and stabilizes training.

#### Distributed Training Techniques
* Zero Redundancy Data Parallelism (ZeRO):
    * Three stages of optimization:
        * ZeRO-1 partitions optimizer states.
        * ZeRO-2 adds gradient partitioning.
        * ZeRO-3 further partitions model parameters.
* Fully Sharded Data Parallel (FSDP):
    * Implements similar concepts to ZeRO.
    * Enables training with larger batch sizes, improving efficiency and scalability.

### Fine-tuning Techniques

#### Parameter-Efficient Fine-Tuning (PEFT)
* [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-lora-a48359cecbfa): Uses low-rank decomposition to update a small subset of model parameters, keeping most pre-trained weights fixed.
* [Prompt Tuning](https://arxiv.org/abs/2104.08691): Inserts learnable prompts into the input data to guide the model's attention and improve performance on specific tasks.
* [Llama-Adapter](https://arxiv.org/abs/2303.16199): Adds learnable adapter modules to LLaMA's attention blocks, allowing for task-specific fine-tuning without modifying the core model architecture.
* [Dynamic Adapters](https://arxiv.org/abs/2405.17741): Combines multiple adapters as a mixture-of-experts model, enabling multi-tasking and preventing forgetting of previously learned information.

#### Data Augmentation
* [AugGPT](https://arxiv.org/abs/2302.13007): Rephrases training samples using ChatGPT to generate diverse and varied input data.
* [Evol-Instruct](https://arxiv.org/abs/2304.12244) [ðŸ“‘](https://ritvik19.medium.com/papers-explained-127-wizardlm-65099705dfa3): Uses multistep revisions to generate increasingly complex and diverse open-domain instructions.
* [Reflection-tuning](https://arxiv.org/abs/2310.11716): Refines both instructions and responses using GPT-4 based on predefined criteria, enhancing data quality and instruction-response consistency.
* [FANNO](https://arxiv.org/abs/2408.01323): Augments instructions and generates responses by incorporating external knowledge sources through retrieval-augmented generation.
* [LLM2LLM](https://arxiv.org/abs/2403.15042): Generates more challenging training samples based on the model's predictions on existing data, pushing the model to learn more robust representations.

## Model Compression Techniques

### Pruning Techniques
* [SparseGPT](https://arxiv.org/abs/2301.00774): Reformulates pruning as a sparse regression problem. Optimizes both remaining and pruned weights using a layer-wise approximate regression solver.
* [Wanda](https://arxiv.org/abs/2306.11695): Considers weights and activations during pruning without requiring weight updates.
* [n:m Pruning Strategy](https://arxiv.org/abs/2102.04010): Prunes exactly n weights out of every m. Balances pruning flexibility and computational efficiency for significant speedups
* [Neuron Sparsity](https://arxiv.org/abs/2210.06313): Observes prevalent neuron sparsity, particularly in feed-forward networks, to guide pruning.
* [Contextual Sparsity](https://arxiv.org/abs/2310.17157): Proposes using smaller neural networks to dynamically prune based on input.
* [Activation Sparsity](https://arxiv.org/abs/2310.04564): Changes activation functions to ReLU and fine-tunes to enhance activation sparsity.

### Quantization
* [GPTQ](https://arxiv.org/abs/2210.17323): Performs layer-wise, weight-only quantization. Uses inverse Hessian matrices to reduce reconstruction error, improving compressed model accuracy.
* [AWQ](https://arxiv.org/abs/2306.00978) and [ZeroQuant](https://arxiv.org/abs/2206.01861): Incorporate activations into quantization to better assess the importance of weights. Allow for more effective optimization during weight quantization, enhancing model efficiency.
* [K/V Cache Quantization](https://arxiv.org/abs/2401.18079): Specifically quantizes Key-Value cache for efficient long-sequence length inference.
* [SmoothQuant](https://arxiv.org/abs/2211.10438): Addresses activation quantization outliers by migrating quantization difficulty from activations to weights. Reduces the impact of outliers in activation distributions, stabilizing the quantization process.
* [SpinQuant](https://arxiv.org/abs/2405.16406): Applies rotation matrices to move outliers to a new space, handling activation quantization challenges. Improves quantization accuracy by transforming outlier values effectively.
* [LLM-QAT](https://arxiv.org/abs/2305.17888) and [EdgeQAT](https://arxiv.org/abs/2402.10787): Use distillation with float16 models to correct quantization error, leading to stronger model performance. Enhances model resilience to quantization-induced degradation by incorporating quantization awareness during training.

### Knowledge Distillation Techniques
* [Classical Knowledge Distillation](https://arxiv.org/abs/1503.02531): Involves training a smaller, efficient "student" model to replicate the behavior of a larger, more complex "teacher" model.
* [BabyLLaMA](https://arxiv.org/abs/2308.02019) and [BabyLLaMA-2](https://arxiv.org/abs/2409.17312): Developed models using a Llama teacher model, showing that distillation from a robust teacher can outperform traditional pre-training on the same dataset.
* [Distillation Loss Modification](https://arxiv.org/abs/2306.08543): Introduces modifications to the distillation loss, enhancing student models' response quality, calibration, and lowering exposure bias.
* [Sequence-Level Distillation with f-Divergences](https://arxiv.org/abs/2307.15190): Uses a generalized f-divergence in the distillation loss to improve sequence-level distillation outcomes.
* [Layer-Wise Distillation with Task-Aware Filters](https://arxiv.org/abs/2210.01351): Employs filters that distill only task-specific knowledge from the teacher, enhancing layer-wise distillation.
* Fusion of Multiple Teacher Models ([Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491), [FuseChat](https://arxiv.org/abs/2408.07990)): Merges output probability distributions from multiple language models as teachers to effectively distill knowledge into smaller models.
* [Universal Logit Distillation Loss](https://arxiv.org/abs/2402.12030): Addresses the limitation of needing the same tokenizer and data availability between teacher and student by introducing a universal logit distillation inspired by optimal transport.
* Distillation Combined with Pruning ([LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796), [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679)): Involves an iterative process of pruning a large model and retraining it with distillation losses to achieve smaller, efficient models.
* [Distillation with Rationales](https://arxiv.org/abs/2305.02301): Adds rationales as extra supervision during distillation, making the process more sample-efficient and improving performance on benchmarks like NLI, Commonsense QA, and arithmetic reasoning.
* Distillation of Reasoning Chains ([Beyond Imitation](https://arxiv.org/abs/2405.19737), [Teaching Small Language Models to Reason](https://arxiv.org/abs/2212.08410), [Large Language Models Are Reasoning Teachers](https://arxiv.org/abs/2212.10071), [Specializing Smaller Language Models towards Multi-Step Reasoning](https://arxiv.org/abs/2301.12726)): Transfers reasoning chains along with labels from a larger model to a smaller one, enhancing arithmetic, multi-step math, symbolic, and commonsense reasoning abilities in the distilled model.