---
markmap:
  color: "#2980b9"
  maxWidth: 400
  initialExpandLevel: 4
---

# [Best Practices and Lessons Learned on Synthetic Data](https://arxiv.org/abs/2404.07503)

## Synthetic Data in Training

### Reasoning

#### Math
- **Math-targeted pre-training**: Models like [Minerva](), [Llemma](), and [DeepSeekMath]() are pre-trained on datasets specifically curated for mathematical tasks.
- **Synthetic question generation**: [WizardMath]() increases complexity in questions and answers using GPT-3.5 to imitate target benchmarks.
- **Bootstrapping via question rephrasing**: [MetaMath]() rewrites questions using techniques like semantic rephrasing, self-verification, and backward reasoning to improve performance.
- **Answer format optimization**: [GAIR-Abel]() shows that paraphrasing the question followed by step-by-step solutions outperforms vanilla answer formats.
- **Data scaling**: [Xwin-Math]() demonstrates that scaling synthetic data up to one million examples benefits models like LLaMA-2 7B.
- **Dataset bundling**: [MMIQC]() combines SFT-style data (rephrased from MetaMath or taken directly) with high-quality pre-training data like OpenWebMath for better performance.
- **Verification of synthetic data**: [AlphaGeometry]() employs a symbolic deduction engine and 100 million synthetic data points for complex geometry problem-solving, achieving results comparable to Olympiad-level performance.

#### Code
- **[CodeRL]():** An actor-critic approach that improves pretrained language models by using feedback signals on synthetic code samples for better code reasoning.
- **[Language Models Can Teach Themselves to Program Better]():** A self-improvement strategy where models generate synthetic puzzle-solution pairs, verified by a real interpreter before being used for fine-tuning.
- **[Learning Performance-Improving Code Edits]():** A framework leveraging simulated environments and adaptation strategies like self-improvement synthetic data generation and CoT prompting for code optimization.
- **[InterCode]():** A framework designed to enhance interactive code generation using reinforcement learning, where code acts as actions and execution feedback as observations.
- **[Reflexion]():** Employs external or simulated linguistic feedback signals to improve code reasoning capabilities of language models.
- **[Code Alpaca]():** A synthetic dataset of 20K code instructions generated via SELF-INSTRUCT applied to ChatGPT across 21 seed tasks.
- **[WizardCoder]():** Introduces Code Evol-Instruct, using heuristic prompts to guide ChatGPT in creating more complex and diverse synthetic data.
- **[Magicoder]():** Developed OSS-INSTRUCT, which generates 75K diverse synthetic instruction samples from open-source code snippets.

#### Other reasoning tasks
- **[Symbol tuning]()**: Augmented natural language datasets by replacing labels with arbitrary symbols, generating 500k+ synthetic examples, improving model performance on unseen in-context learning and algorithmic reasoning tasks.
- **[STaR]()**: Generated synthetic chain-of-thought rationales and filtered out incorrect rationales for finetuning language models to enhance reasoning capabilities.
- **[Mind’s Eye]()**: Trained a text-to-code model with synthetic "text-description → rendering code" data, using a physical engine (MuJoCo) to boost reasoning performance in physics, achieving results comparable to models 100x larger.

### Tool-using and Planning

#### Learning tool-using through synthetic trajectories
- **[LaMDA]()**: Trained on both web documents and synthetic interaction data between crowdworkers and the model, annotated with tool calls, allowing it to learn calculator, search engine, and machine translator usage.
- **[Toolformer]()**: Learns to decide which APIs to call and what arguments to pass through training on template-generated synthetic data.
- **[Galactica]()**: Integrates API-calling data into its pre-training mixture, enhancing tool-use capabilities during pre-training.
- **[ToolAlpaca]()**: Generates a diverse tool-use corpus by simulating a multi-agent environment where agents iteratively select and use tools.

#### Learning to plan in synthetic environments
- **[Planning in Autonomous Machine Intelligence]()**: The agent decomposes complex tasks into subtasks and completes them in a reward-optimal manner.
- **[Synthetic Data as Feedback]()**: Synthetic data, collected from simulators, serves as feedback to aid the agent in planning tasks.
- **[Affordance Awareness]()**: Learning from synthetic data helps agents become aware of affordances, enhancing their ability to act in environments.
- **[Inner Monologue]()**: Uses natural language feedback from simulated environments to teach LLM-based robots planning, improving instruction completion in both simulated and real-world tasks.
- **[VIMA]()**: Develops a multi-modality simulated environment, VIMA-Bench, to generate realistic planning tasks like object rearrangement, supporting a variety of objects and textures.
- **[Voyager]()**: Deploys GPT-4 based agents in Minecraft, finding that synthetic feedback helps unlock new skills faster and makes planning more efficient.

### Multimodality

#### Reverse rendering from vision to text
- **Web-scraped Image-Caption Pairs**: Popular for multimodal alignment, but often noisy and provide only coarse-grained correspondence, limiting fine-grained grounding of images in language.
- **[Pix2Struct]()**: Utilizes web servers to render HTML into website screenshots, training the model to derender masked screenshots back into full HTML code.
- **[MatCha]() and [DePlot]()**: Render tabular data into charts using Python libraries, pretraining models by pairing rendered images with corresponding code or tabular data.
- **[Design2Code]() and [WebSight]()**: Train on synthetically generated HTML and CSS files to convert webpage screenshots into code implementations, achieving good generalization on real-world data.
- **[Unity Perception]()**: Proposes using physics or game engines (e.g., Unity) as synthetic data generators to enhance computer vision research.

#### Multi-modality instruction following
- **[LLaVA]()**: Uses existing image captions to prompt GPT-4 in text-only mode for generating diverse long-form question-response pairs, which can be used in multimodal LLM training.
- **[Svit]()**: Incorporates object bounding box data as image attribute input for multimodal LLMs, fitting into the synthetic data pipeline for image attributes + text.
- **[Llavar]()**: Leverages Optical Character Recognition (OCR) data from images as another source of image attribute information for multimodal LLM training.
- **[UniChart]()**: Integrates data from derendered charts into the multimodal LLM pipeline, using chart attributes and text as inputs for training synthetic data.

### Multilingual
#### Back-translation augmentation
- **Backtranslation for Data Augmentation**: Many multilingual models utilize backtranslation to generate synthetic parallel training data from monolingual sources, enhancing translation performance ([Improving Neural Machine Translation Models with Monolingual Data](), [Improving Neural Machine Translation Models with Monolingual Data](), [Tagged Back-Translation](), [Tagged Back-translation Revisited](), [Data Augmentation for Text Generation Without Any Augmented Data]() [Back-translation for Large-Scale Multilingual Machine Translation]() [Meta Back-Translation](), [On Synthetic Data for Back Translation]()).
- **Sampling Methods Exploration**: Researchers have investigated various sampling methods for backtranslation (beam search, constrained sampling, unconstrained sampling) to assess their comparative effectiveness in generating high-quality synthetic data ([Improving Neural Machine Translation Models with Monolingual Data](), [Understanding Back-Translation at Scale](), [Generalizing Back-Translation in Neural Machine Translation](), [Paraphrasing with Bilingual Parallel Corpora]()).
- **Optimization of Synthetic Data Quality**: [On Synthetic Data for Back Translation]() highlighted the importance of balancing the weight and quality of synthetic data for optimal neural machine translation (NMT) performance, proposing a method to optimize the ratio of search methods alongside a gamma score for improved effectiveness.
- **Limitations of Backtranslation**: Backtranslation's effectiveness can be limited by the quality and diversity of the synthetic data, as poor performance in backtranslation may lead to noisy or insufficiently diverse data, ultimately restricting performance gains ([Improving backtranslation with iterative filtering and data selection for sinhala-english nmt.](), [Improved unsupervised neural machine translation with semantically weighted back translation for morphologically rich and low resource languages.]())

#### Generating multilingual questions and answers at scale
- **Translation of Monolingual QA**: Translating existing monolingual questions and/or answers into other languages to enhance multilingual capabilities ([One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval]()).
- **Cross-lingual Question Generation**: Using Question Generation models to generate synthetic questions in a cross-lingual manner based on provided answers and/or source texts ([Cross-Lingual Training for Automatic Question Generation](), [Cross-Lingual Natural Language Generation via Pre-Training](), [Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering]()).
- **Joint Generation of Multilingual QA**: Focusing on the simultaneous generation of questions and answers across multiple languages for improved flexibility in language model training ([Towards Zero-Shot Multilingual Synthetic Question and Answer Generation for Cross-Lingual Reading Comprehension](), [PAXQA]()).
- **Fine-tuning Multilingual Models**: Fine-tuning a pretrained multilingual T5 model on a combination of QA generation tasks and multilingual masked language modeling to create synthetic QA pairs in various languages ([Towards Zero-Shot Multilingual Synthetic Question and Answer Generation for Cross-Lingual Reading Comprehension]()). 

###  Alignment

#### Instruction Following
- **[Self-Instruct]()**: Utilizes LLMs to generate instruction-following data by imitating a small set of seed samples, expanding the variety of training scenarios.
- **[Stanford Alpaca]()**: Similar to Self-Instruct, this approach generates additional instruction-following examples using LLMs based on initial seed data to enhance training coverage.
- **[Evol-Instruct]()**: Introduces complexity to simple instructions through advanced prompting techniques, aiming to enrich the quality of generated data.
- **[FLAN Dataset Revision]()**: Implements an iterative revision process using LLMs to enhance instructions and responses, resulting in improved model performance across various NLP tasks.
- **[UltraChat]()**: Creates a large-scale synthetic dialogue dataset by employing two ChatGPT Turbo API models, simulating user behavior through carefully crafted prompts.
- **[Synthetic Data for Robustness]()**: Generates synthetic data to mitigate sycophantic tendencies in models, incorporating this data in a finetuning step to promote balanced responses regardless of user opinions.

#### Mitigating hallucination
- **Reward Model in GPT-4**: Leveraged synthetic hallucination data for reinforcement learning, significantly improving performance on the TruthfulQA dataset.
- **[Synthetic Task Design]()**: Created a task for evaluating hallucinations, optimizing outputs by learning a continuous postfix via prefixtuning.
- **[Automated Fact-Checking]()**: Utilized confidence scores to rank factuality of model responses, finetuning with DPO to enhance factual accuracy.

#### Aligning with shared human preference and values
- **Direct Finetuning on Human-Preferred Data**: A straightforward alignment method requiring substantial human annotation, often leading to high costs and inconsistent quality in the annotated samples.
- **Reinforcement Learning from Human Feedback (RLHF)**: Trains a reward model with human data to serve as a proxy for human judgment, optimizing the language model's generation policy based on this feedback.
- **Mixture of Synthetic and Real Human Data**: Combines synthetic data with real human data to enhance the robustness of reward models, providing a more diverse training set ([Scaling laws for reward model overoptimization]()).
- **[Constitutional AI]()**: Utilizes a small set of guiding principles to generate critiques and feedback, allowing for synthetic data to replace real human data in RLHF pipelines, achieving strong performance similar to RLHF baselines.
- **Synthetic Data for Value Alignment**: Offers a cost-effective way to generate large, diverse datasets that simulate various ethical dilemmas, social interactions, and cultural norms, aiding in comprehensive testing of AI models' alignment with human values ([Ultrafeedback](), [Red teaming language models to reduce harms](), [Red Teaming Language Models with Language Models](), [Training Socially Aligned Language Models on Simulated Social Interactions](), [NormBank]()).
- **Identification and Mitigation of Bias and Fairness Issues**: Enables systematic testing to identify biases and fairness issues before real-world deployment, helping to prevent unintended consequences ([Mitigating Political Bias in Language Models through Reinforced Calibration](), [Bias in data-driven artificial intelligence systems](), [Gender Bias in Coreference Resolution](), [Auditing the ai auditors](), [ToolSword]()).
- **Risks of Low-Fidelity Synthetic Data**: Acknowledges limitations in accurately reflecting nuanced human judgment, which may lead to vulnerabilities under specific attacks and deceptive behaviors ([Out of one, many](), [Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](), [Toxicity in ChatGPT](), [The Effects of Reward Misspecification](), [ML Systems Will Have Weird Failure Modes](), [Reward tampering problems and solutions in reinforcement learning]()).

## Synthetic Data in Evaluation
### .
#### Factuality
- Early statistical-based hallucination evaluation methods used n-grams to calculate vocabulary overlap between input and output but failed to account for semantics ([Handling divergent reference texts when evaluating table-to-text generation](), [Towards faithful neural table-to-text generation with content-matching constraints]()).
- Statistical methods were limited as they only considered lexical overlap and couldn't evaluate complex hallucinations ([ Survey of hallucination in natural language generation]()).
- Model-based methods replaced statistical approaches, providing more robustness than token-difference-based methods ([Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering]()).
- Model-based methods can measure hallucination severity but struggle to identify specific factual errors ([Ranking generated summaries by correctness]()).
- Combining LLM generation with random walks on knowledge graphs helps generate synthetic evaluation data by focusing on entities and relations ([FactKB]()).
- The [LongFact]() dataset has been created created for long-form factuality evaluation, using Google Search as grounding and LLM for automated judgement, achieving human-level accuracy at a lower cost ().

#### Safety
- **Red teaming** generates diverse, realistic scenarios to identify unaligned or harmful outputs in AI models ([ Explore, establish, exploit]()).
- **[Red teaming language models with language models]()** used LMs to create 154 high-quality datasets to evaluate other LMs, discovering new inverse scaling issues.
- **[ Sleeper agents]()** leveraged synthetic data to trigger backdoor attacks, revealing deceptive behavior in LMs and limitations of standard safety training.
- **AI assistance** can help scale human oversight in addressing complex and unseen domains ([Measuring progress on scalable oversight for large language models]()).

#### Assisting human evaluation
- **[Alpaca Eval]():** A benchmark that uses GPT-4 as a judge to assess the comprehensive abilities of LM-based chatbots.
- **[MT Bench]():** Another benchmark using GPT-4 to evaluate the capabilities of LM-based chatbots, focusing on various aspects of performance.
- **[CRUXEval]():** A code execution reasoning benchmark with 800 Python functions generated by CodeLLaMA-34B to evaluate coding task performance.
- **[CodeMind]():** A framework that evaluates LLMs on code reasoning abilities across Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR).


## Challenges and Limitations of Synthetic Data
### .
#### Misuse of synthetic data might proliferate misinformation
- **AI systems' capability to generate human-like data**: Models can now produce text, images, songs, and videos, raising concerns about their misuse.
- **Risks of synthetic data impersonating real people**: Synthetic data could be exploited for malicious purposes, such as impersonation, public opinion manipulation, and political interference.
- **Erosion of trust through synthetic data-driven misinformation**: The spread of misinformation generated by synthetic data threatens the credibility of legitimate information sources.
- **Need for ethical guidelines and detection mechanisms**: Researchers, developers, and policymakers must implement clear ethical guidelines and technologies to detect and combat synthetic misinformation .

#### Synthetic data might cause ambiguity in AI alignment
- **Constitutional AI**: The use of synthetic data, like in Constitutional AI, may introduce ambiguity and uncertainty, affecting the model's ability to align with human values.
- **Representation of Human Values**: Synthetic data may fail to accurately represent human values and preferences, potentially leading to misaligned AI behaviors.
- **Bias in Synthetic Data**: Models trained on synthetic data may learn biased information, resulting in misrepresentations of real-world scenarios.
- **Ungrounded Learning**: AI models might be trained on ungrounded data, leading to a lack of real-world applicability.
- **Misrepresentation of Real-World Scenarios**: Synthetic data can misrepresent complex real-world situations, causing AI models to exhibit behaviors that diverge from human expectations.
- **Unintended Consequences**: Misalignment due to synthetic data can lead to unintended or harmful behaviors in AI systems.
- **Interpretability Challenges**: The ambiguity introduced by synthetic data complicates the interpretation of AI decision-making processes, making alignment more difficult.

#### Training with synthetic data makes evaluation decontamination harder
- Synthetic data complicates fair evaluation in model training due to potential overlap with public benchmark test cases.
- Publicly available benchmarks, often sourced from online platforms, risk being included in pre-training data, affecting evaluation fairness.
- Synthetic data use worsens evaluation contamination rather than reducing it, particularly when it rephrases existing benchmark data.
- Token-level decontamination methods, such as min-k% prob, are insufficient for models trained with synthetic data.
- More advanced techniques for evaluation contamination detection are necessary, alongside the creation of protected, proprietary benchmarks for reliable evaluation.

## Directions for Future Work
### .
#### Synthetic data scaling
- **Over-trained small language models:** The performance of models like the Mistral and Gemma series demonstrates that training on large amounts of data, even exceeding the compute-optimal Chinchilla law, leads to impressive results.
- **Synthetic data in training:** It remains an open question whether training with synthetic data can yield similar conclusions, as its quality is often inconsistent compared to real-world data.
- **Scaling laws for synthetic data:** Future research should investigate the optimal balance between the quantity and quality of synthetic data for large-scale language model training.
- **Cost-efficient strategies with synthetic data:** Exploring effective strategies for using synthetic data could lead to more efficient and cost-effective training approaches for large language models.

#### Further improving quality and diversity of synthetic data
- **Generative Adversarial Networks (GANs):** Focus on using GANs to improve the quality of synthetic data by closely mimicking real-world data, with emphasis on controlling specific attributes.
- **Diffusion Models:** Investigate the use of Diffusion Models to generate high-quality synthetic data with more granular control over data attributes.
- **Retrieval Augmented Generation (RAG):** Explore incorporating domain-specific knowledge into synthetic data generation through methods like RAG, ensuring that the data adheres to domain constraints while maintaining quality.
- **Privacy-Preserving Analysis:** Improve synthetic data techniques to support privacy-preserving analysis across fields like healthcare and finance.


#### Towards high-fidelity and more efficient scalable oversight
- **[Debate]():** Simulates social iterations by having AI models engage in a structured debate to expose and correct errors.
- **[Reflection]():** Uses AI models' introspection and self-evaluation to generate synthetic data for oversight by reflecting on their decisions.
- **[Revisions]():** Focuses on iterative correction of AI model outputs to improve oversight using synthetic data.
- **[Comprehensive Scenarios and Modalities]():** Proposes exploring a broader range of scenarios and modalities in synthetic data generation for better oversight.
- **[Narrowed-down Issues]():** Highlights problems arising from simulation of overly specific and narrow scenarios.
- **[Over-simplified Scenes]():** Identifies issues with simulations that are too simplistic to provide meaningful oversight.

#### The emergent self-improvement capability
- **Synthetic Data Quality**: Using the most capable model to generate synthetic data can lead to higher quality outputs compared to less advanced models.
- **Self-Improvement Potential**: Investigating whether models can generate synthetic data superior to their training data, potentially enabling self-improvement.
- **Iterative Learning**: A model could iteratively improve by learning from enhanced synthetic data, leading to continuous performance gains.